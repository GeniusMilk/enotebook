#flashcards 
#八股文
#Anki
#Linux

#myanki


```ActivityHistory
/
```
```toc 
	style: bullet | number | inline (default: bullet) 
	min_depth: number (default: 2) max_depth: number (default: 6) 
	title: string (default: undefined) 
	allow_inconsistent_headings: boolean (default: false) 
	delimiter: string (default: |) 
	varied_style: boolean (default: false) 
```

#### 1.[ Linux面试问题汇总----Linux的I/O模型介绍以及同步异步阻塞非阻塞的区别](https://blog.csdn.net/weixin_41563161/article/details/104086016)

###### 同步、异步
?
同步：用户进程发起IO后，进行就绪判断，轮询内核状态。
异步：用户进程发起IO后，可以做其他事情，等待内核通知。 <!--SR:!2022-08-18,7,230-->

###### 阻塞、非阻塞
?
阻塞：用户进程访问数据时，如果未完成IO，调用的进程一直处于等待状态，直到IO操作完成。
非阻塞：用户进程访问数据时，会马上返回一个状态值，无论是否完成，此时进程可以操作其他事情。 <!--SR:!2022-08-18,7,230-->

Linux下的五种IO模型
?
阻塞I/O（blocking I/O）
非阻塞I/O（nonblocking I/O）
I/O复用(select和poll) （I/O multiplexing）
信号驱动I/O（signal driven I/O (SIGIO)）
异步I/O （asynchronous I/O (the POSIX aio_functions)）
Tip：前四种都是同步，只有最后一种才是异步I/O。 <!--SR:!2022-08-19,10,210-->

###### I/O发生时涉及的对象和阶段

Linux为了OS的安全性等的考虑，==进程是无法直接操作I/O设备的，其必须通过系统调用请求内核来协助完成I/O动作，而内核会为每个I/O设备维护一个buffer。== <!--SR:!2022-08-16,7,224-->

对于一个network I/O (这里我们以read举例)，它会涉及到两个系统对象，一个是调用这个I/O的process (or thread)，另一个就是系统内核(kernel)。当一个read操作发生时，它会经历两个阶段：
![[Pasted image 20220727215251.png]]

I/O  两个阶段
?
(1)用户进程发起请求，内核接收到请求，从I/O设备中获取数据到buffer，等待数据准备 (Waiting for the data to be ready)
(2)将buffer中的数据copy到用户进程的地址空间，即将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) <!--SR:!2022-08-23,11,230-->


#### 2.[ Linux面试问题汇总----文件系统的理解（EXT4，XFS，BTRFS）](https://blog.csdn.net/weixin_41563161/article/details/104086827)

文件系统==主要用于控制所有程序在不使用数据时如何存储数据、如何访问数据以及有什么其它信息（元数据）和数据本身相== <!--SR:!2022-08-16,6,230-->

##### 分区的意义
？
通过将硬盘划分为分区，数据会被分隔以及重组。当事故发生的时候，只有存储在被损坏分区上的数据会被破坏，很大可能上其它分区的数据能得以保留。使用分一区也考虑到了安全和健壮性原因，因此操作系统部分损坏并不意味着整个计算机就有风险或者会受到破坏。

##### 日志文件系统：

 ==日志文件系统是一种即使在断电或者是操作系统崩溃的情况下保证文件系统一致性的途径。==
   
XFS对文件系统元数据提供了日志支持。当文件系统更新时，元数据会在实际的磁盘块被更新之前顺序写入日志。XFS的日志被保存在磁盘块的循环缓冲区上，不会被正常的文件系统操作影响。XFS日志大小的上限是64k个块和128MB中的较大值，下限取决于已存在的文件系统和目录的块的大小。在外置设备上部署日志会浪费超过最大日志大小的空间。XFS日志也可以被存在文件系统的数据区（称为内置日志），或者一个额外的设备上（以减少磁盘操作）。 <!--SR:!2022-08-13,2,150-->

#### XFS的主要特点[]
?
数据完全性、传输特性、可扩展性、传输带宽 <!--SR:!2022-08-13,1,130-->

1.  数据完全性：::采用XFS文件系统，当意想不到的宕机发生后，由于文件系统开启了日志功能，所以你磁盘上的文件不再会意外宕机而遭到破坏了，不论目前文件系统上存储的文件与数据有多少，文件系统都可以根据所记录的日志在很短的时间内迅速恢复磁盘文件内容。 <!--SR:!2022-08-13,1,130-->
s
3.  传输特性：XFS文件系统采用优化算法，日志记录对整体文件操作影响非常小，XFS查询与分配存储空间非常快。xfs文件系统能连续提供快速的反应时间。笔者曾经对XFS、JFS、Ext3、ReiserFS文件系统进行过测试，XFS文件文件系统的性能表现相当出众。

4.  可扩展性：::XFS 是一个全64-bit的文件系统，它可以支持上百万T字节的存储空间。对特大文件及小尺寸文件的支持都表现出众，支持特大数量的目录，最大可支持的文件大 小为263 = 9 x 1018 = 9 exabytes，最大文件系统尺寸为18 exabytes，XFS使用高的表结构(B+树)，保证了文件系统可以快速搜索与快速空间分配。XFS能够持续提供高速操作，文件系统的性能不受目录中目录及文件数量的限制。 <!--SR:!2022-08-13,3,150-->

5.  传输带宽：::XFS 能以接近裸设备I/O的性能存储数据。在单个文件系统的测试中，其吞吐量最高可达7GB每秒，对单个文件的读写操作，其吞吐量可达4GB每秒。 <!--SR:!2022-09-05,26,250-->

##### 为什么要切换文件系统？
?
 ext4 文件系统已经非常稳定，是过去几年中绝大部分发行版的默认选择，但它是基于陈旧的代码开发而来。
 在文件系统层次做到这些能获得更好的性能。 <!--SR:!2022-08-18,9,230-->

#### Btrfs 文件系统
btrfs 有很多不同的叫法，例如 Better FS、Butter FS 或者 B-Tree FS。它是一个几乎完全从头开发的文件系统。btrfs 出现的原因是它的开发者起初希望扩展文件系统的功能使得它包括快照、池化（pooling）、校验以及其它一些功能。虽然和 ext4 无关，它也希望能保留 ext4 中能使消费者和企业受益的功能，并整合额外的能使每个人，尤其是企业受益的功能。对于使用大型软件以及大规模数据库的企业，让多种不同的硬盘看起来一致的文件系统能使他们受益并且使数据整合变得更加简单。删除重复数据能降低数据实际使用的空间，当需要镜像一个单一而巨大的文件系统时使用 btrfs 也能使数据镜像变得简单。


#### 3.文件处理grep,awk,sed这三个命令必知必会

#### 4. IO复用的三种方法（select,poll,epoll）深入理解，包括三者区别，内部原理实现？
#参考链接 [IO多路复用的三种机制Select，Poll，Epoll](https://www.jianshu.com/p/397449cadc9a)
?
==epoll跟select都能提供多路I/O复用的解决方案。在现在的Linux内核里有都能够支持，其中epoll是Linux所特有，而select则应该是POSIX所规定，一般操作系统均有实现。== <!--SR:!2022-08-13,1,130-->

##### select：
select本质上是==通过设置或者检查存放fd标志位的数据结构来进行下一步处理。== <!--SR:!2022-08-19,14,244-->

这样所带来的缺点是：
==（1）单个进程可监视的fd数量被限制，即能监听端口的大小有限：<br>（2）对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低<br>（3）需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。== <!--SR:!2022-08-14,4,204-->

（1）单个进程可监视的fd数量被限制，即能监听端口的大小有限：
一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.
（2）对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低：当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。
（3）需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。

###### select函数解读
```C++
int select(
		int maxfdp1,
		fd_set *readset,
		fd_set *writeset,
		fd_set *exceptset,
		const struct 
		timeval *timeout);
		
```

select参数说明：
？
**int maxfdp1**：:: 指定待测试的文件描述字个数，它的值是待测试的最大描述字加1。   <!--SR:!2022-08-13,1,130-->
**fd_set *readset , fd_set *writeset , fd_set *exceptset**  
`fd_set`::可以理解为一个集合，这个集合中存放的是文件描述符(file descriptor)，即文件句柄。中间的三个参数指定我们要让内核测试读、写和异常条件的文件描述符集合。如果对某一个的条件不感兴趣，就可以把它设为空指针。   <!--SR:!2022-08-14,2,162-->
**const struct timeval *timeout** `timeout`::告知内核等待所指定文件描述符集合中的任何一个就绪可花多少时间。其timeval结构用于指定这段时间的秒数和微秒数。 <!--SR:!2022-08-13,1,130-->

select 返回值
?
**int** 若有就绪描述符返回其数目，若超时则为0，若出错则为-1 <!--SR:!2022-08-14,4,202-->

###### select 运行机制
?
select()的机制中提供一种`fd_set`的数据结构，实际上是一个long类型的数组，每一个数组元素都能与一打开的文件句柄（不管是Socket句柄,还是其他文件或命名管道或设备句柄）建立联系，建立联系的工作由程序员完成，当调用select()时，由内核根据IO状态修改fd_set的内容，由此来通知执行了select()的进程哪一Socket或文件可读。 <!--SR:!2022-08-15,4,182-->

  ###### select 实际使用
  
##### poll：
参考链接：
	[# 深度理解select、poll和epoll](https://blog.csdn.net/davidsguo008/article/details/73556811)
?
poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。 <!--SR:!2022-08-17,8,224-->

==它没有最大连接数的限制，原因是它是基于[[算法与数据结构 Card|链表]]来存储的，==<!--SR:!2022-08-16,6,230-->

但是同样有一个缺点： 
?
（1）大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义；
（2）poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。
<!--SR:!2022-08-13,2,181-->

##### epoll:
?
epoll支持[[Linux Card#水平触发和边缘触发|水平触发]]和[[Linux Card#水平触发和边缘触发|边缘触发]]，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就需态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。 <!--SR:!2022-08-13,3,170-->

###### poll 的函数原型
?
```cpp
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

typedef struct pollfd {
        int fd;                         // 需要被检测或选择的文件描述符
        short events;                   // 对文件描述符fd上感兴趣的事件
        short revents;                  // 文件描述符fd上当前实际发生的事件
} pollfd_t;
```
<!--SR:!2022-08-13,1,142-->

poll参数说明
?
**struct pollfd *fds** `fds`是一个`struct pollfd`类型的数组，用于存放需要检测其状态的socket描述符，并且调用poll函数之后`fds`数组不会被清空；一个`pollfd`结构体表示一个被监视的文件描述符，通过传递`fds`指示 poll() 监视多个文件描述符。其中，结构体的`events`域是监视该文件描述符的事件掩码，由用户来设置这个域，结构体的`revents`域是文件描述符的操作结果事件掩码，内核在调用返回时设置这个域
**nfds_t nfds** 记录数组`fds`中描述符的总数量 <!--SR:!2022-08-13,1,130-->

poll对比select的区别：
?
poll改变了文件描述符集合的描述方式，使用了结构而不是select的结构，使得poll支持的文件描述符集合限制远大于select的1024`pollfd``fd_set` <!--SR:!2022-08-14,2,162-->


poll返回值：
?
**int** 函数返回fds集合中就绪的读、写，或出错的描述符数量，返回0表示超时，返回-1表示出错； <!--SR:!2022-08-13,2,182-->

###### epoll的优点：
?
（1）没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）；<br>（2）效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback数；<br>（3）内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。 <!--SR:!2022-08-13,3,150-->

###### epoll IO多路复用模型实现机制
#线索 
?
通过[[算法与数据结构 Card|TODO红黑树]]和[[算法与数据结构 Card|TODO双链表]]数据结构，并结合回调机制，造就了epoll的高效。 <!--SR:!2022-08-15,4,182-->

epoll IO多路复用模型实现机制
?
epoll的设计和实现与select完全不同。epoll通过在Linux内核中申请一个简易的文件系统[[算法与数据结构 Card|B+tree]]。把原先的select/poll调用分成了3个部分： <!--SR:!2022-08-13,2,182-->

epoll的三部分：
?
1）调用`epoll_create()`建立一个epoll对象(在epoll文件系统中为这个句柄对象分配资源)
2）调用`epoll_ctl()`向epoll对象中添加该连接的套接字
3）调用`epoll_wait()`收集发生的事件的连接
```C++
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```
```

epoll 数据结构：
调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关。eventpoll结构体如下所示：
?
```C++
struct eventpoll{
    ....
    /*红黑树的根节点，这颗树中存储着所有添加到 epoll 中的需要监控的事件*/
    struct rb_root  rbr;
    /*双链表中则存放着将要通过 epoll_wait 返回给用户的满足条件的事件*/
    struct list_head rdlist;
    ....
};
```

每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过`epoll_ctl()`方法向epoll对象中添加进来的事件。这些事件都会挂载在[[算法与数据结构 Card|TODO 红黑树-]]中,O(n)=lgn
?

所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫[[ep_poll_callback|#TODO ep_poll_callback()]]它会将发生的事件添加到[[算法与数据结构 Card|TODO rdlist双链表]]中。

`epitem`结构体
?
在epoll中，对于每一个事件，都会建立一个`epitem`结构体
```C++
struct epitem{
    struct rb_node  rbn;//红黑树节点
    struct list_head    rdllink;//双向链表节点
    struct epoll_filefd  ffd;  //事件句柄信息
    struct eventpoll *ep;    //指向其所属的eventpoll对象
    struct epoll_event event; //期待发生的事件类型
}

```

当调用`epoll_wait()`检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。==如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。==
						<center>
	<img style="border-radius: 0.3125em;
	 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="http://static.open-open.com/lib/uploadImg/20140911/20140911103834_133.jpg"> 
	 <br> 
	 <div style="color:orange; border-bottom: 1px solid #d9d9d9; 
	 display: inline-block;
	 color: #999; 
	 padding: 2px;">epoll数据结构示意图</div>
</center>


###### epoll的用法【三步】
?
第一步：`epoll_create()`系统调用。此调用返回一个句柄，之后所有的使用都依靠这个句柄来标识。
第二步：`epoll_ctl()`系统调用。通过此调用向epoll对象中添加、删除、修改感兴趣的事件，返回0标识成功，返回-1表示失败。
第三部：`epoll_wait()`系统调用。通过此调用收集收集在epoll监控中已经发生的事件。

###### 水平触发和边缘触发
?
-   水平触发（LT）：默认工作模式，即当`epoll_wait()`检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；下次调用epoll_wait时，会再次通知此事件
-   边缘触发（ET）： 当`epoll_wait()`检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件。如果不处理，下次调用`epoll_wait()`时，不会再次通知此事件。（直到你做了某些操作导致该描述符变成未就绪状态了，也就是说边缘触发只在状态由未就绪变为就绪时只通知一次）。

  
 

##### select、poll、epoll 区别总结：
[# IO多路复用的三种机制Select，Poll，Epoll](https://www.jianshu.com/p/397449cadc9a)
?
#线索
1、支持一个进程所能打开的最大连接数<br>2、FD剧增后带来的IO效率问题<br>3、 消息传递方式

###### 1、支持一个进程所能打开的最大连接数
?
| 方法       |     |
| ------ | --- |
| select |  单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是32*32，同理64位机器上FD_SETSIZE为32*64），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试。     |
| poll   |  poll本质上和select没有区别，但是它==没有最大连接数的限制，原因是它是基于链表来存储的==|
| epoll       |   虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接，2G内存的机器可以打开20万左右的连接     | <!--SR:!2022-08-09,3,224-->

###### 2、FD剧增后带来的IO效率问题
?
| 方法       |     |
| ------ | --- |
| select | 因为每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的“线性下降性能问题”。    |
| poll   | 因为每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的“线性下降性能问题”。    |
| epoll       | 因为epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，所以==在活跃socket较少的情况下，使用epoll没有前面两者的线性下降的性能问题，但是所有socket都很活跃的情况下，可能会有性能问题。==    | <!--SR:!2022-08-09,3,224-->

###### 3、 消息传递方式
?
| 方法       |     |
| ------ | --- |
| select | 内核需要将消息传递到用户空间，都需要内核拷贝动作    |
| poll   | 内核需要将消息传递到用户空间，都需要内核拷贝动作    |
| epoll       | epoll通过内核和用户空间共享一块内存来实现的。    | <!--SR:!2022-08-10,10,250-->

 
#### 6.查询进程占用CPU的命令（注意要了解到used，buf，***代表意义）
[Linux查看系统负载常用命令](https://blog.csdn.net/qq_36357820/article/details/76606113)
##### 1. top命令查看linux负载：
##### 2. uptime查看linux负载
##### 3. w查看linux负载：
##### 4. vmstat查看linux负载

####  7. linux的其他常见命令（kill，find，cp等等）
#### 8.shell脚本用法
#### 9.硬连接和软连接的区别
#### 10.文件权限怎么看（rwx）
#### 11.文件的三种时间（mtime, atime，ctime），分别在什么时候会改变
#### 12.Linux监控网络带宽的命令，查看特定进程的占用网络资源情况命令
监控总体带宽使用――nload、bmon、slurm、bwm-ng、cbm、speedometer和netload
监控总体带宽使用（批量式输出）――vnstat、ifstat、dstat和collectl
#### 每个套接字连接的带宽使用――iftop、iptraf、tcptrack、pktstat、netwatch和trafshow
#### 每个进程的带宽使用――nethogs
#### 13.Cache和Buffer的区别

1. Cache：缓存区，是高速缓存，是位于CPU和主内存之间的容量较小但速度很快的存储器，因为CPU的速度远远高于主内存的速度，CPU从内存中读取数据需等待很长的时间，而  Cache保存着CPU刚用过的数据或循环使用的部分数据，这时从Cache中读取数据会更快，减少了CPU等待的时间，提高了系统的性能。

    Cache==并不是缓存文件的，而是缓存块的(块是I/O读写最小的单元)；Cache一般会用在I/O请求上，如果多个进程要访问某个文件，可以把此文件读入Cache中，这样下一个进程获取CPU控制权并访问此文件直接从Cache读取，提高系统性能。== <!--SR:!2022-08-10,10,250-->

2. Buffer：缓冲区，用于存储速度不同步的设备或优先级不同的设备之间传输数据；通过buffer可以减少进程间通信需要等待的时间，当存储速度快的设备与存储速度慢的设备进行通信时，存储慢的数据先把数据存放到buffer，达到一定程度存储快的设备再读取buffer的数据，在此期间存储快的设备CPU可以干其他的事情。

Buffer：==一般是用在写入磁盘的，例如：某个进程要求多个字段被读入，当所有要求的字段被读入之前已经读入的字段会先放到buffer中。== <!--SR:!2022-08-09,3,224-->

#TODO buffer cache（即buff）、page cache（即cache）和cpu cache的区别

#### 14.Linux 系统中如何查看日志 (常用命令）
#### （17）Linux的GDB调试
[GDB调试命令详解](https://blog.csdn.net/qq_28351609/article/details/114855630)

#### （18）coredump

##### coredump是什么 
![[C++ Card#什么是coredump]]
##### coredump是什么 怎么才能coredump

##### coredump产生的条件
1. shell资源控制限制，使用 `ulimit -c` 命令查看shell执行程序时的资源 ，如果为0，则不会产生coredump。可以用`ulimit -c unlimited `设置为不限大小。
2. 读写越界，包括：数组访问越界，指针指向错误的内存，字符串读写越界
3. 使用了线程不安全的函数，读写未加锁保护
4. 错误使用指针转换
5. 堆栈溢出

##### [[C++ Card#（41） 遇到coredump要怎么调试|遇到coredump要怎么调试]]
![[C++ Card#（41） 遇到coredump要怎么调试]]

#### Linux [[OS Card#（1） 进程与线程|线程]]相关
#####  Linux理论上最多可以创建多少个进程，影响因素
?
2^15 = 32768个, 因为进程的pid是用pid_t来表示的，pid_t的最大值是32768.所以理论上最多有32768个进程。

影响因素
#线索 
?
pid_t 的大小、系统本身参数/etc/security/limits.conf大小、资源限制

限制1：::既然系统使用pid_t表示进程号，那么最大进程数不能超过pid_t类型的最大值吧

限制2：::使用命令ulimit -u查看系统中限制的最大进程数，我的机器上是65535。/etc/security/limits.conf里面是硬限制，ulimit -u是软限制，内核参数kernel.pid_max也做了限制。

限制3：::受系统资源限制，创建一个新进程会消耗系统资源，最主要的就是内存，有人做过测试，在创建6千多个进程时，程序运行得很慢，通过vmstat命令观察，发现swap内存的置入置出很频繁，可以判断是由于内存不足，使用虚拟内存，导致频繁的IO操作，让测试代码变得很慢，所以创建过多进程时，系统的内存是重要衡量的一个方面。

###### Linux一个进程可以创建多少线程和什么有关
进程最多可以创建的线程数是根==据分配给调用栈的大小，以及操作系统（32位和64位不同）==共同决定的。Linux32位下是300多个。

 linux 系统中单个进程的最大线程数有其最大的限制 PTHREAD_THREADS_MAX，这个限制可以在 `/usr/include/bits/local_lim.h` 中查看，对linuxthreads 这个值一般是 1024，对于 nptl 则没有硬性的限制，仅仅受限于系统的资源， 这个系统的资源主要就是线程的 stack 所占用的内存，用 ulimit -s 可以查看默认的线程栈大小，一般情况下，这个值是 8M。

### 死锁

#### 死锁的产生

##### 死锁产生的主要原因
#线索 
？
（1） 因为系统资源不足。
（2） 进程运行推进的顺序不合适
（3） 资源分配不当等

##### 死锁产生的必要条件
#线索 
？
互斥、占有并请求、不可剥夺、循环等待


（1） 互斥：一个资源每次只能被一个进程使用。
（2） 占有并请求：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
（3） 不可剥夺:进程已获得的资源，在末使用完之前，不能强行剥夺。
（4） 循环等待:若干进程之间形成一种头尾相接的循环等待资源关系。

#### 死锁的恢复
#线索 
？
重新启动、终止进程、剥夺资源、进程回退

###### 重新启动：
?
是最简单、最常用的死锁消除方法，但代价很大，因为在此之前所有进程已经完成的计算工作都将付之东流，不仅包括死锁的全部进程，也包括未参与死锁的全部进程。

###### 终止进程(process termination)：
?
终止参与死锁的进程并回收它们所占资源。
    (1) 一次性全部终止；
    (2) 逐步终止(优先级，代价函数)

###### 剥夺资源(resource preemption)：
?
剥夺死锁进程所占有的全部或者部分资源。
    (1) 逐步剥夺：一次剥夺死锁进程所占有的一个或一组资源，如果死锁尚未解除再继续剥夺，直至死锁解除为止。
    (2) 一次剥夺：一次性地剥夺死锁进程所占有的全部资源。

###### 进程回退(rollback):
?
让参与死锁的进程回退到以前没有发生死锁的某个点处，并由此点开始继续执行，希望进程交叉执行时不再发生死锁。但是系统开销很大：
    (1) 要实现“回退”，必须“记住”以前某一点处的现场，而现场随着进程推进而动态变化，需要花费大量时间和空间。
    (2) 一个回退的进程应当“挽回”它在回退点之间所造成的影响，如修改某一文件，给其它进程发送消息等，这些在实现时是难以做到的
